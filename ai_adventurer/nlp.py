#!/usr/bin/env python
"""The NLP language functionality.

"""

import logging
import time

import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold


logger = logging.getLogger(__name__)


class NotAuthenticatedError(Exception):
    pass


class NLPThread(object):
    """The general NLP functionality.

    Subclass for the NLP variants, e.g. using external APIs.

    """

    def __init__(self, secrets=None, extra=None, modelname=None):
        self.secrets = secrets
        self.modelname = modelname

    def prompt(self, text=None):
        """Subclass for the specifig NLP generation.

        Adds the previous dialog to the prompt, for giving context.
        """
        logger.debug("Prompt given: %s", text)


class MockNLPThread(NLPThread):
    """Simulating NLP behaviour"""

    replies = (
        "The guy asked around.",
        "The girl looked at you.",
        '"What?!" she asked, looking at you.',
        '"Well well well," he said.',
    )

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def prompt(self, text=None):
        super().prompt(text)
        import random

        response = random.choice(self.replies)
        return response


class LocalNLPThread(NLPThread):
    """NLP prompt thread executed on a model file generated by keras"""

    def __init__(self, *args, extra=None, **kwargs):
        super().__init__(*args, **kwargs)
        keras_file = extra
        self.model = self._load_model(keras_file)

    def _load_model(self, keras_file):
        import keras
        # import keras_nlp
        model = keras.saving.load_model(keras_file, compile=True)
        return model

    def prompt(self, text):
        super().prompt(text)

        if isinstance(text, (list, tuple)):
            text = "\n".join(text)

        output = self._generate(text)

        # Remove the pretext, so only the new ouptut is returned
        output = output[len(text):].strip()
        return output

    def _generate(self, pretext):
        logger.debug("Generating with prompt: '%s'", pretext)
        logger.debug("Prompt length: %s", len(pretext))

        start = time.time()
        output = self.model.generate(
            pretext, max_length=min(1024, len(pretext) + 40)
        ).strip()
        end = time.time()

        logger.debug("Returned answer: %s", output)
        logger.debug("Time elapsed: %.2f", end - start)
        return output


class HuggingfaceNLPThread(NLPThread):
    """Download a model from HuggingFace and prompt locally"""

    def __init__(self, *args, extra=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.modelid = extra
        self.model = self._load_model(self.modelid)

    def _load_model(self, modelid):
        from transformers import pipeline
        import torch
        logger.debug(f"Try to load HF model {modelid!r}")
        return pipeline("text-generation", model=modelid,
                        model_kwargs={"torch_dtype": torch.bfloat16},
                        device_map="auto")

    def prompt(self, text):
        super().prompt(text)

        if isinstance(text, (list, tuple)):
            text = "\n".join(text)
        output = self._generate(text)
        return output

    def _generate(self, pretext):
        logger.debug("Generating with prompt: '%s'", pretext)
        logger.debug("Prompt length: %s", len(pretext))
        start = time.time()
        raw = self.model(pretext,
                         return_full_text=False,
                         max_new_tokens=30)
        output = raw[0]["generated_text"]
        end = time.time()

        logger.debug("Returned answer: %s", output)
        logger.debug("Time elapsed: %.2f", end - start)
        return output


class OpenAINLPThread(NLPThread):

    openai_model = "gpt-4o-mini"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        api_key = self.secrets["DEFAULT"]["openai-key"]
        if api_key == "CHANGEME":
            raise NotAuthenticatedError(
                "Invalid API key - see " +
                "https://platform.openai.com/api-keys")

        from openai import OpenAI
        self.client = OpenAI(api_key=api_key)
        if not self.modelname:
            self.modelname = self.openai_model
        logger.debug(f"Model: {self.modelname}")

    def prompt(self, text):
        super().prompt(text)
        # Reformat the prompt to follow OpenAIs specs:
        new_text = []
        for t in text:
            new_text.append({"role": "user", "content": t})

        answer = self.client.chat.completions.create(
            model=self.modelname,
            messages=new_text,
            stream=False,
        )
        return answer.choices[0].message.content


class GeminiNLPThread(NLPThread):

    google_model = "gemini-1.5-flash"

    # Google's tresholds are very sensitive, so need to adjust these. Keep low
    # for now.
    safety_settings = {
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT:
            HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HARASSMENT:
            HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH:
            HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:
            HarmBlockThreshold.BLOCK_NONE,
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        api_key = self.secrets["DEFAULT"]["gemini-key"]
        if api_key == "CHANGEME":
            raise NotAuthenticatedError(
                "Invalid API key - see " +
                "https://aistudio.google.com/app/apikey")

        genai.configure(api_key=api_key)
        if not self.modelname:
            self.modelname = self.google_model

        self.client = genai.GenerativeModel(
            self.modelname, safety_settings=self.safety_settings
        )
        logger.debug(f"Model: {self.modelname}")

    def prompt(self, text):
        super().prompt(text)
        response = self.client.generate_content(
            contents=text,
        )
        try:
            answer = response.text
            logger.debug("Response from Gemini: '%s'", answer)
        except ValueError:
            logger.debug(response.prompt_feedback)
            logger.debug("Blocked by Gemini: '%r'", response)
            answer = str(response.prompt_feedback)
            logger.debug("Returning: '%r'", answer)

        return answer


class MistralNLP(NLPThread):

    mistral_model = "open-mistral-nemo"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        api_key = self.secrets["DEFAULT"]["mistral-key"]
        if api_key == "CHANGEME":
            raise NotAuthenticatedError(
                "Invalid API key - see " +
                "https://console.mistral.ai/api-keys/")

        from mistralai import Mistral
        self.client = Mistral(api_key=api_key)
        if not self.modelname:
            self.modelname = self.mistral_model

    def prompt(self, text):
        super().prompt(text)

        # Reformat the prompt to follow OpenAIs specs:
        new_text = []
        for t in text:
            new_text.append({"role": "user", "content": t})

        logger.debug(f"model {self.modelname}")

        import mistralai
        try:
            answer = self.client.chat.complete(
                model=self.modelname,
                messages=new_text,
                stream=False,
            )
        except mistralai.models.sdkerror.SDKError as e:
            if e.status_code == 401:
                raise NotAuthenticatedError(e)
            logger.critical(e, exc_info=True)
            raise e
        return answer.choices[0].message.content


models = {
    "gemini-1.5-flash": GeminiNLPThread,
    "gemini-1.5-pro": GeminiNLPThread,
    "gemini-1.0-pro": GeminiNLPThread,
    "gpt-4o-mini": OpenAINLPThread,
    "gpt-4o": OpenAINLPThread,
    "open-mistral-nemo": MistralNLP,
    "mistral-large-latest": MistralNLP,
    "mock": MockNLPThread,
    # TODO: would probably also need a file name
    "local": LocalNLPThread,
    "huggingface": HuggingfaceNLPThread,
}

model2apikey = {
    GeminiNLPThread: 'gemini-key',
    OpenAINLPThread: 'openai-key',
    MistralNLP: 'mistral-key',
}


def get_nlp_class(model):
    return models[model]
